# Start
pySpark 

# Read the csv file
diamonds = spark.read.csv("diamonds.csv", header=True, inferSchema=True)

# How many diamonds do we have?
diamonds.count()

# What is the schema of the DataFrame?
diamonds.printSchema()

# Shows the first lines
diamonds.show()

# Show basic statistics
diamonds.describe("carat","price").show()

# Select the column "cut" and show the distinct values
diamonds.select('cut').distinct().show()

# Filter only expensive diamonds
expensive_diamonds = diamonds.where(diamonds["price"] > 15000)

# We can use different data transformations like
# - select
# - where / filter (like where in SQL)
# - grouBy
# - agregations like avg, sum, max, min, mean, count
# - limit
# - join
# - drop (drops columns or rows)
# - create new columns
df1 = diamonds.groupBy("cut", "color").avg("price")

# Show the first 100 rows
df1.show(100)

# Open http://seneca1.lehre.hwr-berlin.de:18088/
# Go to your last task
# Open Event Time Line
# Joining two dataframes
df2 = df1.join(diamonds, on='color', how='inner').select("avg(price)", "carat")

# Create temporary SQL table out of a dataframe
diamonds.createOrReplaceTempView("diamonds")

# Create a dataframe out of a SQL query
df3 = spark.sql("select color, avg(price) as average_price from diamonds group by color order
by avg(price) desc")

# show the results
df3.show()

# Write the result into a HDFS folder "color"
df3.write.csv("color")

# What cut has the highest average price?
# TODO:cut = diamonds.groupBy('cut').avg('price')

# Exit spark
exit()

# List the content of the HDFS folder "color"
hdfs dfs -ls color

# Concatenate (cat) all parts of the results and show them
hadoop fs -cat "color/*"

# Get the merged output to the local file system and rename it to color.csv
hadoop fs -getmerge color/ color.csv

# Show the content of color.csv
less color.csv



